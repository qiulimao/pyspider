{
  "name": "Pyspider",
  "tagline": "A nicer pyspider and powerful spider ",
  "body": "forked from pyspider [![Build Status]][Travis CI] [![Coverage Status]][Coverage] [![Try]][Demo]\r\n========\r\n\r\nA Powerful Spider(Web Crawler) System in Python. **[TRY IT NOW!][Demo]**\r\n\r\n- Write script in Python\r\n- Powerful WebUI with script editor, task monitor, project manager and result viewer\r\n- [MySQL](https://www.mysql.com/), [MongoDB](https://www.mongodb.org/), [Redis](http://redis.io/), [SQLite](https://www.sqlite.org/), [Elasticsearch](https://www.elastic.co/products/elasticsearch); [PostgreSQL](http://www.postgresql.org/) with [SQLAlchemy](http://www.sqlalchemy.org/) as database backend\r\n- [RabbitMQ](http://www.rabbitmq.com/), [Beanstalk](http://kr.github.com/beanstalkd/), [Redis](http://redis.io/) and [Kombu](http://kombu.readthedocs.org/) as message queue\r\n- Task priority, retry, periodical, recrawl by age, etc...\r\n- Distributed architecture, Crawl Javascript pages, Python 2&3, etc...\r\n\r\nTutorial: [http://docs.pyspider.org/en/latest/tutorial/](http://docs.pyspider.org/en/latest/tutorial/)  \r\nDocumentation: [http://docs.pyspider.org/](http://docs.pyspider.org/)  \r\nRelease notes: [https://github.com/binux/pyspider/releases](https://github.com/binux/pyspider/releases)  \r\n\r\nSample Code \r\n-----------\r\n\r\n```python\r\nfrom pyspider.libs.base_handler import *\r\n\r\n\r\nclass Handler(BaseHandler):\r\n    crawl_config = {\r\n    }\r\n\r\n    @every(minutes=24 * 60)\r\n    def on_start(self):\r\n        self.crawl('http://scrapy.org/', callback=self.index_page)\r\n\r\n    @config(age=10 * 24 * 60 * 60)\r\n    def index_page(self, response):\r\n        \"\"\" you can use response.xpath now,just like scrapy,\r\n            It also support extract,just like scrapy's extrace,but you need to \r\n            from pyspider.libs.response import Response\r\n            Response.extract(response.xpath(\"//a\"))\r\n            this static method will help you extract content of <a>\r\n        \"\"\"\r\n        for url in response.xpath(\"//a[starts-with(./@href,'http://')]/@href\"):\r\n            self.crawl(each.url, callback=self.detail_page)\r\n\r\n    def detail_page(self, response):\r\n        return {\r\n            \"url\": response.url,\r\n            \"title\": response.doc('title').text(),\r\n        }\r\n```\r\n\r\nAnd the web center\r\n------------------\r\n\r\n[![Demo][Demo Img]][Demo]\r\n\r\n\r\nInstallation\r\n------------\r\n\r\n* you need to run `git clone https://github.com/qiulimao/pyspider.git` then `$python setup.py install`\r\n* run command `pyspider`, visit [http://localhost:5000/](http://localhost:5000/)\r\n\r\nQuickstart: [http://docs.pyspider.org/en/latest/Quickstart/](http://docs.pyspider.org/en/latest/Quickstart/)\r\n\r\nContribute\r\n----------\r\n\r\n* Use Angularjs to rebuild the ui module\r\n* more resonable way to save data in mongodb\r\n* extra method on parsing the html\r\n* parameters optimized\r\n\r\n\r\nTODO\r\n----\r\n\r\n### next\r\n\r\n- [ ] read more source code\r\n- [ ] make better ui\r\n- [ ] better user authentic\r\n\r\n\r\n### more\r\n\r\n- [x] edit script with vim via [WebDAV](http://en.wikipedia.org/wiki/WebDAV)\r\n- [x] binux is realy COW B!!!!\r\n\r\n\r\nLicense\r\n-------\r\nLicensed under the Apache License, Version 2.0\r\n\r\n\r\n[Build Status]:         https://img.shields.io/travis/binux/pyspider/master.svg?style=flat\r\n[Travis CI]:            https://travis-ci.org/binux/pyspider\r\n[Coverage Status]:      https://img.shields.io/coveralls/binux/pyspider.svg?branch=master&style=flat\r\n[Coverage]:             https://coveralls.io/r/binux/pyspider\r\n[Try]:                  https://img.shields.io/badge/try-pyspider-blue.svg?style=flat\r\n[Demo]:                 http://demo.pyspider.org/\r\n[Demo Img]:             http://www.getqiu.com/static/image/pyspider-angularjs.png\r\n[Issue]:                https://github.com/binux/pyspider/issues\r\n[User Group]:           https://groups.google.com/group/pyspider-users\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}